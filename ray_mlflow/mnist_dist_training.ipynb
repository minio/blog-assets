{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import tempfile\n",
    "from time import time\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "import mlflow\n",
    "from mlflow import MlflowClient\n",
    "import ray\n",
    "from ray import train\n",
    "from ray.train import Checkpoint, ScalingConfig\n",
    "from ray.train.torch import TorchTrainer\n",
    "import requests\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "import data_utilities as du\n",
    "import torch_utilities as tu\n",
    "\n",
    "device = torch.device('cpu')  #torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ray Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_ray():\n",
    "    runtime_env = {\n",
    "        'pip': ['minio', 'mlflow']\n",
    "    }\n",
    "    ray.init(runtime_env=runtime_env)\n",
    "\n",
    "\n",
    "def log_metric(base_url: str, run_id: str, metric: Dict[str, float]) -> int:\n",
    "    '''Log a metric dict for the given run.'''\n",
    "    base_url = f'{base_url}/api/2.0/mlflow'\n",
    "    url = base_url + '/runs/log-metric'\n",
    "    payload = {\n",
    "        \"run_id\": run_id,\n",
    "        \"key\": metric[\"key\"],\n",
    "        \"value\": metric[\"value\"],\n",
    "        \"timestamp\": mlflow.utils.time.get_current_time_millis(),\n",
    "        \"step\": metric[\"step\"],\n",
    "    }\n",
    "    r = requests.post(url, json=payload)\n",
    "    return r.status_code\n",
    "\n",
    "\n",
    "def get_minio_run_config():\n",
    "    import s3fs\n",
    "    import pyarrow.fs\n",
    "\n",
    "    s3_fs = s3fs.S3FileSystem(\n",
    "        key = os.environ['MINIO_ACCESS_KEY'],\n",
    "        secret = os.environ['MINIO_SECRET_ACCESS_KEY'],\n",
    "        endpoint_url = 'http://localhost:9000' #os.environ['MINIO_URL']\n",
    "    )\n",
    "    custom_fs = pyarrow.fs.PyFileSystem(pyarrow.fs.FSSpecHandler(s3_fs))\n",
    "\n",
    "    run_config = train.RunConfig(storage_path='ray-train', storage_filesystem=custom_fs)\n",
    "    return run_config\n",
    "\n",
    "\n",
    "def train_model(model: tu.MNISTModel, train_data: ray.data.dataset.MaterializedDataset, training_parameters: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    loss_func = nn.NLLLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=training_parameters['lr'], momentum=training_parameters['momentum'])\n",
    "\n",
    "    for epoch in range(training_parameters['epochs']):\n",
    "        total_loss = 0\n",
    "        batch_count = 0\n",
    "        for batch in train_data.iter_torch_batches(batch_size=training_parameters['batch_size_per_worker']):\n",
    "            # Get the images and labels from the batch.\n",
    "            images, labels = batch['X'], batch['y']\n",
    "            labels = labels.type(torch.LongTensor)   # casting to long\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Flatten MNIST images into a 784 long vector.\n",
    "            images = images.view(images.shape[0], -1)\n",
    "\n",
    "            # Training pass\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(images)\n",
    "            loss = loss_func(output, labels)\n",
    "\n",
    "            # This is where the model learns by backpropagating\n",
    "            loss.backward()\n",
    "\n",
    "            # And optimizes its weights here\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            batch_count +=1\n",
    "            \n",
    "        #ray.train.report({'training_loss': total_loss/len(loader)})\n",
    "        print(\"Epoch {} - Training loss: {}\".format(epoch+1, total_loss/batch_count))\n",
    "\n",
    "    #return training_metrics\n",
    "\n",
    "\n",
    "def train_func_per_worker(training_parameters):\n",
    "    logger = du.create_logger()\n",
    "    \n",
    "    # Train the model and log training metrics.\n",
    "    model = tu.MNISTModel(training_parameters['input_size'], training_parameters['hidden_sizes'], \n",
    "                          training_parameters['output_size'])\n",
    "    model = ray.train.torch.prepare_model(model)\n",
    "\n",
    "    # Get the dataset shard for the training worker.\n",
    "    train_data_shard = train.get_dataset_shard('train')\n",
    "\n",
    "    loss_func = nn.NLLLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=training_parameters['lr'], momentum=training_parameters['momentum'])\n",
    "\n",
    "    metrics = {}\n",
    "    batch_size_per_worker = training_parameters['batch_size_per_worker']\n",
    "    for epoch in range(training_parameters['epochs']):\n",
    "        total_loss = 0\n",
    "        batch_count = 0\n",
    "        for batch in train_data_shard.iter_torch_batches(batch_size=batch_size_per_worker):\n",
    "            # Get the images and labels from the batch.\n",
    "            images, labels = batch['X'], batch['y']\n",
    "            labels = labels.type(torch.LongTensor)   # casting to long\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Flatten MNIST images into a 784 long vector.\n",
    "            images = images.view(images.shape[0], -1)\n",
    "        \n",
    "            # Training pass\n",
    "            optimizer.zero_grad()            \n",
    "            output = model(images)\n",
    "\n",
    "            loss = loss_func(output, labels)\n",
    "            \n",
    "            # This is where the model learns by backpropagating\n",
    "            loss.backward()\n",
    "            \n",
    "            # And optimizes its weights here\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            batch_count += 1\n",
    "\n",
    "        metrics = {'training_loss': total_loss/batch_count}\n",
    "        checkpoint = None\n",
    "        if train.get_context().get_world_rank() == 0:\n",
    "            temp_dir = os.path.join(os.getcwd(), 'checkpoint')\n",
    "            torch.save(model.module.state_dict(), os.path.join(temp_dir, 'mnist_model.pt'))\n",
    "            checkpoint = Checkpoint.from_directory(temp_dir)\n",
    "            mlflow_metric = {}\n",
    "            mlflow_metric['key'] = 'training_loss'\n",
    "            mlflow_metric['value'] = loss.item()\n",
    "            mlflow_metric['step'] = epoch+1\n",
    "            log_metric(training_parameters['mlflow_base_url'], training_parameters['run_id'], mlflow_metric)\n",
    "\n",
    "        train.report(metrics, checkpoint=checkpoint)\n",
    "        #logger.info('Sending metrics:')\n",
    "        #logger.info(metrics)\n",
    "\n",
    "\n",
    "def distributed_training(training_parameters, num_workers: int, use_gpu: bool):\n",
    "    logger = du.create_logger()\n",
    "\n",
    "    # Setup mlflow to point to our server.\n",
    "    experiment_name = 'MLFlow - Ray test'\n",
    "    run_name = 'Testing Epoch metrics'\n",
    "    mlflow_base_url = 'http://localhost:5001/'\n",
    "    mlflow.set_tracking_uri(mlflow_base_url)\n",
    "    active_experiment = mlflow.set_experiment(experiment_name)\n",
    "    active_run = mlflow.start_run(run_name=run_name)\n",
    "    training_parameters['mlflow_base_url'] = mlflow_base_url\n",
    "    training_parameters['run_id'] = active_run.info.run_id\n",
    "    # Log parameters\n",
    "    mlflow.log_params(training_parameters)\n",
    "    \n",
    "    logger.info('Initializing Ray.')\n",
    "    initialize_ray()\n",
    "\n",
    "    train_data, test_data, load_time_sec = du.get_ray_dataset(training_parameters)\n",
    "\n",
    "    # Scaling configuration\n",
    "    scaling_config = ScalingConfig(num_workers=num_workers, use_gpu=use_gpu)\n",
    "\n",
    "    # Initialize a Ray TorchTrainer\n",
    "    start_time = time()\n",
    "    trainer = TorchTrainer(\n",
    "        train_loop_per_worker=train_func_per_worker,\n",
    "        train_loop_config=training_parameters,\n",
    "        datasets={'train': train_data},\n",
    "        scaling_config=scaling_config,\n",
    "        run_config=get_minio_run_config() # train.RunConfig(storage_path=os.getcwd(), name=\"ray_experiments\") \n",
    "    )\n",
    "    result = trainer.fit()\n",
    "    training_time_sec = (time()-start_time)\n",
    "\n",
    "    logger.info(result)\n",
    "    logger.info(f'Load Time (in seconds) = {load_time_sec}')\n",
    "    logger.info(f'Training Time (in seconds) = {training_time_sec}')\n",
    "    \n",
    "    model = tu.MNISTModel(training_parameters['input_size'], training_parameters['hidden_sizes'], training_parameters['output_size'])\n",
    "    with result.checkpoint.as_directory() as checkpoint_dir:\n",
    "        model.load_state_dict(torch.load(os.path.join(checkpoint_dir, \"model.pt\")))\n",
    "    tu.test_model(model, test_data)\n",
    "\n",
    "    # Shut down Ray    \n",
    "    ray.shutdown()\n",
    "    # End the run\n",
    "    mlflow.end_run()\n",
    "\n",
    "\n",
    "def local_training(training_parameters):\n",
    "    logger = du.create_logger()\n",
    "\n",
    "    train_data, test_data, load_time_sec = du.get_ray_dataset(training_parameters)\n",
    "\n",
    "    # Train the model and log training metrics.\n",
    "    model = tu.MNISTModel(training_parameters['input_size'], training_parameters['hidden_sizes'], training_parameters['output_size'])\n",
    "    model.to(device)\n",
    "    logger.info(f'Model created on device {device}')\n",
    "    \n",
    "    start_time = time()\n",
    "    train_model(model, train_data, training_parameters)\n",
    "    training_time_sec = (time()-start_time)\n",
    "    logger.info(f'Load Time (in seconds) = {load_time_sec}')\n",
    "    logger.info(f'Training Time (in seconds) = {training_time_sec}')\n",
    "\n",
    "    tu.test_model(model, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TemporaryDirectory '/var/folders/_5/jt7lb09d49n9qscq4l2m3sph0000gn/T/tmpf2ryzk7t'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tempfile.TemporaryDirectory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/keithpij/Documents/code/ds-engine/ray_train/checkpoint'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_dir = os.path.join(os.getcwd(), 'checkpoint')\n",
    "temp_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/var/folders/_5/jt7lb09d49n9qscq4l2m3sph0000gn/T/tmpk6j4jvp_/mnist_model.pt\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    print(os.path.join(temp_dir, 'mnist_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-12-28 16:54:29</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:18.13        </td></tr>\n",
       "<tr><td>Memory:      </td><td>17.2/32.0 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 3.0/10 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  \n",
       "  Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                           </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TorchTrainer_a1faa_00000</td><td style=\"text-align: right;\">           1</td><td>/Users/keithpij/ray_results/TorchTrainer_2023-12-28_16-54-10/TorchTrainer_a1faa_00000_0_2023-12-28_16-54-10/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TorchTrainer_a1faa_00000</td><td>ERROR   </td><td>127.0.0.1:3768</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TorchTrainer pid=3768)\u001b[0m Starting distributed worker processes: ['3776 (127.0.0.1)', '3777 (127.0.0.1)']\n",
      "\u001b[36m(RayTrainWorker pid=3776)\u001b[0m Setting up process group for: env:// [rank=0, world_size=2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3521 2023-12-28 16:54:28,839 | WARNING | A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: fffffffffffffffff51d271bce81c48bac52ed7301000000 Worker ID: 0555d1f1f09f24457a2e4133c1e42c08de74a81f4dae28654d8b3e1c Node ID: 98cb03f14dbf918f84461f4854bc898d90ddf009889655605032e720 Worker IP address: 127.0.0.1 Worker port: 55142 Worker PID: 3776 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3776)\u001b[0m [/Users/runner/work/pytorch/pytorch/pytorch/third_party/gloo/gloo/transport/uv/libuv.h:596] uv_accept: invalid argument\n",
      "\u001b[36m(RayTrainWorker pid=3776)\u001b[0m *** SIGABRT received at time=1703800468 ***\n",
      "\u001b[36m(RayTrainWorker pid=3776)\u001b[0m PC: @        0x1897be0dc  (unknown)  __pthread_kill\n",
      "\u001b[36m(RayTrainWorker pid=3776)\u001b[0m     @        0x108161fc8  (unknown)  absl::lts_20220623::WriteFailureInfo()\n",
      "\u001b[36m(RayTrainWorker pid=3776)\u001b[0m     @        0x108161d14  (unknown)  absl::lts_20220623::AbslFailureSignalHandler()\n",
      "\u001b[36m(RayTrainWorker pid=3776)\u001b[0m     @        0x189825a24  (unknown)  _sigtramp\n",
      "\u001b[36m(RayTrainWorker pid=3776)\u001b[0m     @        0x1897f5cc0  (unknown)  pthread_kill\n",
      "\u001b[36m(RayTrainWorker pid=3776)\u001b[0m     @        0x189701a40  (unknown)  abort\n",
      "\u001b[36m(RayTrainWorker pid=3776)\u001b[0m     @        0x304e7d58c  (unknown)  gloo::transport::uv::Device::listenCallback()\n",
      "\u001b[36m(RayTrainWorker pid=3776)\u001b[0m     @        0x304e8e254  (unknown)  gloo::transport::uv::libuv::Emitter<>::Handler<>::publish()\n",
      "\u001b[36m(RayTrainWorker pid=3776)\u001b[0m     @        0x304ef44f8  (unknown)  uv__server_io\n",
      "\u001b[36m(RayTrainWorker pid=3776)\u001b[0m     @        0x304ef8b8c  (unknown)  uv__io_poll\n",
      "\u001b[36m(RayTrainWorker pid=3776)\u001b[0m     @        0x304eea24c  (unknown)  uv_run\n",
      "\u001b[36m(RayTrainWorker pid=3776)\u001b[0m     @        0x304e7ffe4  (unknown)  std::__1::__thread_proxy<>()\n",
      "\u001b[36m(RayTrainWorker pid=3776)\u001b[0m     @        0x1897f6034  (unknown)  _pthread_start\n",
      "\u001b[36m(RayTrainWorker pid=3776)\u001b[0m     @        0x1897f0e3c  (unknown)  thread_start\n",
      "\u001b[36m(RayTrainWorker pid=3776)\u001b[0m [2023-12-28 16:54:28,749 E 3776 61034] logging.cc:361: *** SIGABRT received at time=1703800468 ***\n",
      "\u001b[36m(RayTrainWorker pid=3776)\u001b[0m [2023-12-28 16:54:28,749 E 3776 61034] logging.cc:361: PC: @        0x1897be0dc  (unknown)  __pthread_kill\n",
      "\u001b[36m(RayTrainWorker pid=3776)\u001b[0m [2023-12-28 16:54:28,749 E 3776 61034] logging.cc:361:     @        0x108161fc8  (unknown)  absl::lts_20220623::WriteFailureInfo()\n",
      "\u001b[36m(RayTrainWorker pid=3776)\u001b[0m [2023-12-28 16:54:28,750 E 3776 61034] logging.cc:361:     @        0x108161d2c  (unknown)  absl::lts_20220623::AbslFailureSignalHandler()\n",
      "\u001b[36m(RayTrainWorker pid=3776)\u001b[0m [2023-12-28 16:54:28,750 E 3776 61034] logging.cc:361:     @        0x189825a24  (unknown)  _sigtramp\n",
      "\u001b[36m(RayTrainWorker pid=3776)\u001b[0m [2023-12-28 16:54:28,750 E 3776 61034] logging.cc:361:     @        0x1897f5cc0  (unknown)  pthread_kill\n",
      "\u001b[36m(RayTrainWorker pid=3776)\u001b[0m [2023-12-28 16:54:28,750 E 3776 61034] logging.cc:361:     @        0x189701a40  (unknown)  abort\n",
      "\u001b[36m(RayTrainWorker pid=3776)\u001b[0m [2023-12-28 16:54:28,750 E 3776 61034] logging.cc:361:     @        0x304e7d58c  (unknown)  gloo::transport::uv::Device::listenCallback()\n",
      "\u001b[36m(RayTrainWorker pid=3776)\u001b[0m [2023-12-28 16:54:28,751 E 3776 61034] logging.cc:361:     @        0x304e8e254  (unknown)  gloo::transport::uv::libuv::Emitter<>::Handler<>::publish()\n",
      "\u001b[36m(RayTrainWorker pid=3776)\u001b[0m [2023-12-28 16:54:28,751 E 3776 61034] logging.cc:361:     @        0x304ef44f8  (unknown)  uv__server_io\n",
      "\u001b[36m(RayTrainWorker pid=3776)\u001b[0m [2023-12-28 16:54:28,751 E 3776 61034] logging.cc:361:     @        0x304ef8b8c  (unknown)  uv__io_poll\n",
      "\u001b[36m(RayTrainWorker pid=3776)\u001b[0m [2023-12-28 16:54:28,752 E 3776 61034] logging.cc:361:     @        0x304eea24c  (unknown)  uv_run\n",
      "\u001b[36m(RayTrainWorker pid=3776)\u001b[0m [2023-12-28 16:54:28,753 E 3776 61034] logging.cc:361:     @        0x304e7ffe4  (unknown)  std::__1::__thread_proxy<>()\n",
      "\u001b[36m(RayTrainWorker pid=3776)\u001b[0m [2023-12-28 16:54:28,753 E 3776 61034] logging.cc:361:     @        0x1897f6034  (unknown)  _pthread_start\n",
      "\u001b[36m(RayTrainWorker pid=3776)\u001b[0m [2023-12-28 16:54:28,753 E 3776 61034] logging.cc:361:     @        0x1897f0e3c  (unknown)  thread_start\n",
      "\u001b[36m(RayTrainWorker pid=3776)\u001b[0m Fatal Python error: Aborted\n",
      "\u001b[36m(RayTrainWorker pid=3776)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3776)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3776)\u001b[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_osx, psutil._psutil_posix, setproctitle, yaml._yaml, uvloop.loop, ray._raylet, charset_normalizer.md, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, pyarrow.lib, pyarrow._hdfsio, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pandas._libs.ops, pyarrow._compute, pandas._libs.arrays, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.indexing, pandas._libs.index, pandas._libs.internals, pandas._libs.join, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, pyarrow._fs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, pyarrow._parquet, pydantic.typing, pydantic.errors, pydantic.version, pydantic.utils, pydantic.class_validators, pydantic.config, pydantic.color, pydantic.datetime_parse, pydantic.validators, pydantic.networks, pydantic.types, pydantic.json, pydantic.error_wrappers, pydantic.fields, pydantic.parse, pydantic.schema, pydantic.main, pydantic.dataclasses, pydantic.annotated_types, pydantic.decorator, pydantic.env_settings, pydantic.tools, pydantic, pyarrow._json, torch._C, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special (total: 100)\n",
      "\u001b[36m(TorchTrainer pid=3768)\u001b[0m The actor died unexpectedly before finishing this task.\n",
      "\u001b[36m(TorchTrainer pid=3768)\u001b[0m \tclass_name: RayTrainWorker\n",
      "\u001b[36m(TorchTrainer pid=3768)\u001b[0m \tactor_id: f51d271bce81c48bac52ed7301000000\n",
      "\u001b[36m(TorchTrainer pid=3768)\u001b[0m \tpid: 3776\n",
      "\u001b[36m(TorchTrainer pid=3768)\u001b[0m \tnamespace: a66c6be2-bc99-4b42-b502-0f5f054120ca\n",
      "\u001b[36m(TorchTrainer pid=3768)\u001b[0m \tip: 127.0.0.1\n",
      "\u001b[36m(TorchTrainer pid=3768)\u001b[0m The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[36m(TorchTrainer pid=3768)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TorchTrainer pid=3768)\u001b[0m   File \"/tmp/ray/session_2023-12-28_16-53-44_416516_3521/runtime_resources/pip/30ccc3bdd6303bf86ae8cbb6b7399e5d93b5bd26/virtualenv/lib/python3.10/site-packages/ray/train/_internal/backend_executor.py\", line 196, in start\n",
      "\u001b[36m(TorchTrainer pid=3768)\u001b[0m     self._backend.on_start(self.worker_group, self._backend_config)\n",
      "\u001b[36m(TorchTrainer pid=3768)\u001b[0m   File \"/tmp/ray/session_2023-12-28_16-53-44_416516_3521/runtime_resources/pip/30ccc3bdd6303bf86ae8cbb6b7399e5d93b5bd26/virtualenv/lib/python3.10/site-packages/ray/train/torch/config.py\", line 198, in on_start\n",
      "\u001b[36m(TorchTrainer pid=3768)\u001b[0m     ray.get(setup_futures)\n",
      "\u001b[36m(TorchTrainer pid=3768)\u001b[0m   File \"/tmp/ray/session_2023-12-28_16-53-44_416516_3521/runtime_resources/pip/30ccc3bdd6303bf86ae8cbb6b7399e5d93b5bd26/virtualenv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "\u001b[36m(TorchTrainer pid=3768)\u001b[0m     return fn(*args, **kwargs)\n",
      "\u001b[36m(TorchTrainer pid=3768)\u001b[0m   File \"/tmp/ray/session_2023-12-28_16-53-44_416516_3521/runtime_resources/pip/30ccc3bdd6303bf86ae8cbb6b7399e5d93b5bd26/virtualenv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "\u001b[36m(TorchTrainer pid=3768)\u001b[0m     return func(*args, **kwargs)\n",
      "\u001b[36m(TorchTrainer pid=3768)\u001b[0m   File \"/tmp/ray/session_2023-12-28_16-53-44_416516_3521/runtime_resources/pip/30ccc3bdd6303bf86ae8cbb6b7399e5d93b5bd26/virtualenv/lib/python3.10/site-packages/ray/_private/worker.py\", line 2565, in get\n",
      "\u001b[36m(TorchTrainer pid=3768)\u001b[0m     raise value\n",
      "\u001b[36m(TorchTrainer pid=3768)\u001b[0m ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n",
      "\u001b[36m(TorchTrainer pid=3768)\u001b[0m \tclass_name: RayTrainWorker\n",
      "\u001b[36m(TorchTrainer pid=3768)\u001b[0m \tactor_id: f51d271bce81c48bac52ed7301000000\n",
      "\u001b[36m(TorchTrainer pid=3768)\u001b[0m \tpid: 3776\n",
      "\u001b[36m(TorchTrainer pid=3768)\u001b[0m \tnamespace: a66c6be2-bc99-4b42-b502-0f5f054120ca\n",
      "\u001b[36m(TorchTrainer pid=3768)\u001b[0m \tip: 127.0.0.1\n",
      "\u001b[36m(TorchTrainer pid=3768)\u001b[0m The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[36m(TorchTrainer pid=3768)\u001b[0m Failure occurred during startup. Restarting all workers and attempting to startup again.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3521 2023-12-28 16:54:29,012 | ERROR | Trial task failed for trial TorchTrainer_a1faa_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/keithpij/Documents/code/ds-engine/.venv/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/Users/keithpij/Documents/code/ds-engine/.venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/Users/keithpij/Documents/code/ds-engine/.venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/keithpij/Documents/code/ds-engine/.venv/lib/python3.10/site-packages/ray/_private/worker.py\", line 2563, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=3768, ip=127.0.0.1, actor_id=ce39f17c48211ac6304f239001000000, repr=TorchTrainer)\n",
      "  File \"/tmp/ray/session_2023-12-28_16-53-44_416516_3521/runtime_resources/pip/30ccc3bdd6303bf86ae8cbb6b7399e5d93b5bd26/virtualenv/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 342, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/tmp/ray/session_2023-12-28_16-53-44_416516_3521/runtime_resources/pip/30ccc3bdd6303bf86ae8cbb6b7399e5d93b5bd26/virtualenv/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 91, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/tmp/ray/session_2023-12-28_16-53-44_416516_3521/runtime_resources/pip/30ccc3bdd6303bf86ae8cbb6b7399e5d93b5bd26/virtualenv/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 115, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/Users/keithpij/Documents/code/ds-engine/.venv/lib/python3.10/site-packages/ray/train/base_trainer.py\", line 797, in _trainable_func\n",
      "    super()._trainable_func(self._merged_config)\n",
      "  File \"/Users/keithpij/Documents/code/ds-engine/.venv/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 332, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/Users/keithpij/Documents/code/ds-engine/.venv/lib/python3.10/site-packages/ray/train/base_trainer.py\", line 707, in train_func\n",
      "    trainer.training_loop()\n",
      "  File \"/tmp/ray/session_2023-12-28_16-53-44_416516_3521/runtime_resources/pip/30ccc3bdd6303bf86ae8cbb6b7399e5d93b5bd26/virtualenv/lib/python3.10/site-packages/ray/train/data_parallel_trainer.py\", line 448, in training_loop\n",
      "    backend_executor.start()\n",
      "  File \"/tmp/ray/session_2023-12-28_16-53-44_416516_3521/runtime_resources/pip/30ccc3bdd6303bf86ae8cbb6b7399e5d93b5bd26/virtualenv/lib/python3.10/site-packages/ray/train/_internal/backend_executor.py\", line 203, in start\n",
      "    self._increment_failures()\n",
      "  File \"/tmp/ray/session_2023-12-28_16-53-44_416516_3521/runtime_resources/pip/30ccc3bdd6303bf86ae8cbb6b7399e5d93b5bd26/virtualenv/lib/python3.10/site-packages/ray/train/_internal/backend_executor.py\", line 719, in _increment_failures\n",
      "    raise failure\n",
      "TypeError: exceptions must derive from BaseException\n",
      "3521 2023-12-28 16:54:29,091 | WARNING | Experiment checkpoint syncing has been triggered multiple times in the last 30.0 seconds. A sync will be triggered whenever a trial has checkpointed more than `num_to_keep` times since last sync or if 300 seconds have passed since last sync. If you have set `num_to_keep` in your `CheckpointConfig`, consider increasing the checkpoint frequency or keeping more checkpoints. You can supress this warning by changing the `TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S` environment variable.\n",
      "3521 2023-12-28 16:54:29,096 | ERROR | Trials did not complete: [TorchTrainer_a1faa_00000]\n",
      "3521 2023-12-28 16:54:29,097 | INFO | Total run time: 18.22 seconds (18.05 seconds for the tuning loop).\n"
     ]
    },
    {
     "ename": "TrainingFailedError",
     "evalue": "The Ray Train run failed. Please inspect the previous error messages for a cause. After fixing the issue (assuming that the error is not caused by your own application logic, but rather an error such as OOM), you can restart the run from scratch or continue this run.\nTo continue this run, you can use: `trainer = TorchTrainer.restore(\"/Users/keithpij/ray_results/TorchTrainer_2023-12-28_16-54-10\")`.\nTo start a new run that will retry on training failures, set `train.RunConfig(failure_config=train.FailureConfig(max_failures))` in the Trainer's `run_config` with `max_failures > 0`, or `max_failures = -1` for unlimited retries.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRayTaskError(TypeError)\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;31mRayTaskError(TypeError)\u001b[0m: \u001b[36mray::_Inner.train()\u001b[39m (pid=3768, ip=127.0.0.1, actor_id=ce39f17c48211ac6304f239001000000, repr=TorchTrainer)\n  File \"/tmp/ray/session_2023-12-28_16-53-44_416516_3521/runtime_resources/pip/30ccc3bdd6303bf86ae8cbb6b7399e5d93b5bd26/virtualenv/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 342, in train\n    raise skipped from exception_cause(skipped)\n  File \"/tmp/ray/session_2023-12-28_16-53-44_416516_3521/runtime_resources/pip/30ccc3bdd6303bf86ae8cbb6b7399e5d93b5bd26/virtualenv/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 91, in run\n    self._ret = self._target(*self._args, **self._kwargs)\n  File \"/tmp/ray/session_2023-12-28_16-53-44_416516_3521/runtime_resources/pip/30ccc3bdd6303bf86ae8cbb6b7399e5d93b5bd26/virtualenv/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 115, in <lambda>\n    training_func=lambda: self._trainable_func(self.config),\n  File \"/Users/keithpij/Documents/code/ds-engine/.venv/lib/python3.10/site-packages/ray/train/base_trainer.py\", line 797, in _trainable_func\n    super()._trainable_func(self._merged_config)\n  File \"/Users/keithpij/Documents/code/ds-engine/.venv/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 332, in _trainable_func\n    output = fn()\n  File \"/Users/keithpij/Documents/code/ds-engine/.venv/lib/python3.10/site-packages/ray/train/base_trainer.py\", line 707, in train_func\n    trainer.training_loop()\n  File \"/tmp/ray/session_2023-12-28_16-53-44_416516_3521/runtime_resources/pip/30ccc3bdd6303bf86ae8cbb6b7399e5d93b5bd26/virtualenv/lib/python3.10/site-packages/ray/train/data_parallel_trainer.py\", line 448, in training_loop\n    backend_executor.start()\n  File \"/tmp/ray/session_2023-12-28_16-53-44_416516_3521/runtime_resources/pip/30ccc3bdd6303bf86ae8cbb6b7399e5d93b5bd26/virtualenv/lib/python3.10/site-packages/ray/train/_internal/backend_executor.py\", line 203, in start\n    self._increment_failures()\n  File \"/tmp/ray/session_2023-12-28_16-53-44_416516_3521/runtime_resources/pip/30ccc3bdd6303bf86ae8cbb6b7399e5d93b5bd26/virtualenv/lib/python3.10/site-packages/ray/train/_internal/backend_executor.py\", line 719, in _increment_failures\n    raise failure\nTypeError: exceptions must derive from BaseException",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTrainingFailedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 24\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# training configuration\u001b[39;00m\n\u001b[1;32m     13\u001b[0m training_parameters \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size_per_worker\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m64\u001b[39m \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_workers,\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msmoke_test_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     22\u001b[0m     }\n\u001b[0;32m---> 24\u001b[0m \u001b[43mdistributed_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_gpu\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#local_training(training_parameters)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 167\u001b[0m, in \u001b[0;36mdistributed_training\u001b[0;34m(training_parameters, num_workers, use_gpu)\u001b[0m\n\u001b[1;32m    159\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m    160\u001b[0m trainer \u001b[38;5;241m=\u001b[39m TorchTrainer(\n\u001b[1;32m    161\u001b[0m     train_loop_per_worker\u001b[38;5;241m=\u001b[39mtrain_func_per_worker,\n\u001b[1;32m    162\u001b[0m     train_loop_config\u001b[38;5;241m=\u001b[39mtraining_parameters,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    165\u001b[0m     run_config\u001b[38;5;241m=\u001b[39mget_minio_run_config() \u001b[38;5;66;03m# train.RunConfig(storage_path=os.getcwd(), name=\"ray_experiments\") \u001b[39;00m\n\u001b[1;32m    166\u001b[0m )\n\u001b[0;32m--> 167\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m training_time_sec \u001b[38;5;241m=\u001b[39m (time()\u001b[38;5;241m-\u001b[39mstart_time)\n\u001b[1;32m    170\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(result)\n",
      "File \u001b[0;32m~/Documents/code/ds-engine/.venv/lib/python3.10/site-packages/ray/train/base_trainer.py:618\u001b[0m, in \u001b[0;36mBaseTrainer.fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    614\u001b[0m result \u001b[38;5;241m=\u001b[39m result_grid[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39merror:\n\u001b[1;32m    616\u001b[0m     \u001b[38;5;66;03m# Raise trainable errors to the user with a message to restore\u001b[39;00m\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;66;03m# or configure `FailureConfig` in a new run.\u001b[39;00m\n\u001b[0;32m--> 618\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TrainingFailedError(\n\u001b[1;32m    619\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([restore_msg, TrainingFailedError\u001b[38;5;241m.\u001b[39m_FAILURE_CONFIG_MSG])\n\u001b[1;32m    620\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mresult\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merror\u001b[39;00m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mTrainingFailedError\u001b[0m: The Ray Train run failed. Please inspect the previous error messages for a cause. After fixing the issue (assuming that the error is not caused by your own application logic, but rather an error such as OOM), you can restart the run from scratch or continue this run.\nTo continue this run, you can use: `trainer = TorchTrainer.restore(\"/Users/keithpij/ray_results/TorchTrainer_2023-12-28_16-54-10\")`.\nTo start a new run that will retry on training failures, set `train.RunConfig(failure_config=train.FailureConfig(max_failures))` in the Trainer's `run_config` with `max_failures > 0`, or `max_failures = -1` for unlimited retries."
     ]
    }
   ],
   "source": [
    "# Load the credentials and connection information.\n",
    "with open('credentials.json') as f:\n",
    "    credentials = json.load(f)\n",
    "\n",
    "os.environ['MINIO_URL'] = credentials['url']\n",
    "os.environ['MINIO_ACCESS_KEY'] = credentials['accessKey']\n",
    "os.environ['MINIO_SECRET_ACCESS_KEY'] = credentials['secretKey']\n",
    "\n",
    "num_workers = 2\n",
    "use_gpu = False\n",
    "\n",
    "# training configuration\n",
    "training_parameters = {\n",
    "    'batch_size_per_worker': 64 // num_workers,\n",
    "    'epochs': 3,\n",
    "    'input_size': 784,\n",
    "    'hidden_sizes': [1024, 1024, 1024, 1024],\n",
    "    'lr': 0.025,\n",
    "    'momentum': 0.5,\n",
    "    'output_size': 10,\n",
    "    'smoke_test_size': 0\n",
    "    }\n",
    "\n",
    "distributed_training(training_parameters, num_workers, use_gpu)\n",
    "#local_training(training_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()\n",
    "mlflow.end_run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
